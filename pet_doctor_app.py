import streamlit as st
from openai import OpenAI

# --- 1. é…ç½®åŒºåŸŸ (å¼€å‘è€…ä¿®æ”¹è¿™é‡Œ) ---
# ä½ å¯ä»¥åœ¨è¿™é‡ŒæŒ‡å®šæƒ³è®©è®¿å®¢ä½¿ç”¨çš„æ¨¡å‹
MODEL_CONFIG = {
    # é€‰é¡¹: "deepseek" æˆ– "aliyun" (é€šä¹‰åƒé—®) æˆ– "openai"
    "provider": "aliyun",

    # æ¨¡å‹å‚æ•°é…ç½®
    "deepseek": {
        "base_url": "https://api.deepseek.com",
        "model": "deepseek-chat"
    },
    "aliyun": {
        "base_url": "https://dashscope.aliyuncs.com/compatible-mode/v1",
        "model": "qwen-plus"  # æ€§ä»·æ¯”é«˜
    },
    "openai": {
        "base_url": "https://api.openai.com/v1",
        "model": "gpt-3.5-turbo"
    }
}

# --- é¡µé¢é…ç½® ---
st.set_page_config(page_title="AI å® ç‰©å¥åº·åŠ©æ‰‹", page_icon="ğŸ¾", layout="centered")

# --- 2. è·å– API Key (ä» Secrets å®‰å…¨è¯»å–) ---
try:
    # å°è¯•ä» Streamlit Secrets è¯»å–åä¸º "API_KEY" çš„å¯†é’¥
    api_key = st.secrets["sk-e6e07d9befb14961bfa38ae0d280a40a"]
except FileNotFoundError:
    st.error("âŒ æœªæ‰¾åˆ°å¯†é’¥é…ç½®ï¼è¯·åœ¨æœ¬åœ°åˆ›å»º .streamlit/secrets.toml æˆ–åœ¨äº‘ç«¯è®¾ç½® Secretsã€‚")
    st.stop()
except KeyError:
    st.error("âŒ é…ç½®æ–‡ä»¶ä¸­ç¼ºå°‘ 'API_KEY' å­—æ®µã€‚")
    st.stop()

# è·å–å½“å‰é…ç½®çš„æ¨¡å‹ä¿¡æ¯
current_conf = MODEL_CONFIG[MODEL_CONFIG["provider"]]

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.title("ğŸ¾ æ™ºèƒ½å…½åŒ»åŠ©ç†")
    st.markdown(f"**å½“å‰çŠ¶æ€**: ğŸŸ¢ åœ¨çº¿\n\n**æ¥å…¥æ¨¡å‹**: `{current_conf['model']}`")
    st.info("æœ¬æœåŠ¡ç”± AI é©±åŠ¨ï¼Œæä¾›å…è´¹å’¨è¯¢ã€‚å»ºè®®ä»…ä¾›å‚è€ƒï¼Œæ€¥é‡ç—‡è¯·åŠ¡å¿…çº¿ä¸‹å°±åŒ»ï¼")

    st.divider()
    st.subheader("ğŸ“ å® ç‰©æ¡£æ¡ˆ")
    pet_type = st.selectbox("å® ç‰©ç±»å‹", ["çŒ«å’ª ğŸ±", "ç‹—ç‹— ğŸ¶", "å¼‚å®  ğŸ°", "å…¶ä»–"])
    pet_age = st.slider("å® ç‰©å¹´é¾„ (å²)", 0, 20, 2)
    pet_weight = st.number_input("å® ç‰©ä½“é‡ (kg)", 0.1, 50.0, 5.0)

# --- åˆå§‹åŒ–èŠå¤© ---
if "messages" not in st.session_state:
    st.session_state.messages = [
        {"role": "system", "content": f"""
        ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œã€å¯Œæœ‰åŒæƒ…å¿ƒçš„AIå…½åŒ»ä¸“å®¶ã€‚
        å½“å‰æ¥è¯Šå¯¹è±¡ï¼š{pet_type}ï¼Œ{pet_age}å²ï¼Œ{pet_weight}kgã€‚

        è¯·éµå¾ªä»¥ä¸‹åŸåˆ™ï¼š
        1. **åˆæ­¥è¯Šæ–­**ï¼šç»™å‡º3ç§æœ€å¯èƒ½çš„ç–¾ç—…æˆ–åŸå› ã€‚
        2. **é£é™©é¢„è­¦**ï¼šå¦‚æœæ˜¯æ€¥ç—‡ï¼Œå¿…é¡»åŠ ç²—å¼ºè°ƒ**ç«‹åˆ»å»åŒ»é™¢**ã€‚
        3. **æŠ¤ç†å»ºè®®**ï¼šæä¾›å®¶åº­æŠ¤ç†æªæ–½ã€‚
        4. **å¥½ç‰©æ¨è**ï¼šæ¨è1-2æ¬¾é€šç”¨ç”¨å“ï¼ˆä¸æ¨èä¸‰æ— å“ç‰Œï¼‰ã€‚
        5. **è¯­æ°”**ï¼šæ¸©æŸ”ã€ä¸“ä¸šã€‚
        """}
    ]

# --- ä¸»ç•Œé¢ ---
st.title("ğŸ¥ AI å® ç‰©åœ¨çº¿é—®è¯Šå°")
st.caption("å…è´¹å…¬ç›Šç‰ˆ | è¯·è¯¦ç»†æè¿°ç—‡çŠ¶")

# åˆå§‹åŒ–å®¢æˆ·ç«¯
client = OpenAI(api_key=api_key, base_url=current_conf["base_url"])

# æ˜¾ç¤ºå†å²
for message in st.session_state.messages:
    if message["role"] != "system":
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

# å¤„ç†è¾“å…¥
if prompt := st.chat_input("æˆ‘å®¶çŒ«å’ªä»Šå¤©æ—©ä¸Šåäº†é»„æ°´..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        full_response = ""
        try:
            stream = client.chat.completions.create(
                model=current_conf["model"],
                messages=st.session_state.messages,
                stream=True,
                temperature=0.7
            )
            for chunk in stream:
                if chunk.choices[0].delta.content is not None:
                    full_response += chunk.choices[0].delta.content
                    message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)
            st.session_state.messages.append({"role": "assistant", "content": full_response})
        except Exception as e:
            st.error(f"è¿æ¥ç¹å¿™ï¼Œè¯·ç¨åå†è¯•: {e}")
